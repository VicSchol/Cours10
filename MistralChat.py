import streamlit as st
import os
import logging
from dotenv import load_dotenv

# --- FIX OPENMP (Indispensable sur Windows pour FAISS) ---
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# --- NOUVEAUX IMPORTS LANGCHAIN & MISTRAL V1 ---
from langchain_mistralai.chat_models import ChatMistralAI
from langchain_core.messages import HumanMessage, AIMessage

# --- Importations depuis vos modules ---
try:
    from utils.config import (
        MISTRAL_API_KEY, MODEL_NAME, SEARCH_K,
        APP_TITLE, NAME
    )
    from utils.vector_store import VectorStoreManager
except ImportError as e:
    st.error(f"Erreur d'importation: {e}. V√©rifiez votre structure de dossiers.")
    st.stop()

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration du mod√®le LangChain ---
if not MISTRAL_API_KEY:
    st.error("Cl√© API Mistral non trouv√©e dans le fichier .env.")
    st.stop()

# Initialisation du LLM via le wrapper LangChain
llm = ChatMistralAI(
    mistral_api_key=MISTRAL_API_KEY,
    model=MODEL_NAME,
    temperature=0.1
)

# --- Chargement du Vector Store (mis en cache) ---
@st.cache_resource 
def get_vector_store_manager():
    try:
        manager = VectorStoreManager()
        if manager.index is None:
            return None
        return manager
    except Exception as e:
        logging.error(f"Erreur chargement VectorStoreManager: {e}")
        return None

vector_store_manager = get_vector_store_manager()

# --- Prompt Syst√®me pour RAG ---
SYSTEM_PROMPT = """Tu es 'NBA Analyst AI', un assistant expert sur la ligue NBA.
Ta mission est de r√©pondre aux questions en te basant sur les documents fournis.

CONTEXTE FOURNI :
{context_str}

QUESTION DU FAN :
{question}

R√âPONSE DE L'ANALYSTE NBA :"""

# --- Interface Utilisateur Streamlit ---
st.title(APP_TITLE)
st.caption(f"Assistant virtuel pour {NAME} | Propuls√© par LangChain & Mistral")



# Initialisation de l'historique de conversation
if "messages" not in st.session_state:
    st.session_state.messages = [
        AIMessage(content=f"Bonjour ! Je suis votre analyste IA pour la {NAME}. Posez-moi vos questions !")
    ]

# Affichage des messages de l'historique
for message in st.session_state.messages:
    role = "user" if isinstance(message, HumanMessage) else "assistant"
    with st.chat_message(role):
        st.write(message.content)

# Zone de saisie utilisateur
if prompt := st.chat_input("Posez votre question..."):
    # 1. Ajouter et afficher le message de l'utilisateur
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.write(prompt)

    # 2. V√©rifier le Vector Store
    if vector_store_manager is None:
        st.error("Base de connaissances indisponible.")
        st.stop()

    # 3. Logique RAG (R√©cup√©ration et G√©n√©ration)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status = message_placeholder.info("üîç Recherche dans la base documentaire...")

        try:
            # Recherche de contexte
            search_results = vector_store_manager.search(prompt, k=SEARCH_K)
            
            if search_results:
                context_str = "\n\n".join([
                    f"Source: {res['metadata'].get('filename', 'Doc')} | Extrait: {res['text']}" 
                    for res in search_results
                ])
                status.info("‚úçÔ∏è Analyse des documents et r√©daction...")
            else:
                context_str = "Aucune information trouv√©e dans les documents."
                status.warning("‚ö†Ô∏è Pas de documents trouv√©s, r√©ponse bas√©e sur mes connaissances g√©n√©rales.")

            # Construction du prompt final
            final_prompt = SYSTEM_PROMPT.format(context_str=context_str, question=prompt)

            # Appel au LLM via LangChain (.invoke remplace client.chat)
            response = llm.invoke(final_prompt)
            response_content = response.content

            # Affichage de la r√©ponse
            message_placeholder.write(response_content)
            
            # Sauvegarde dans l'historique
            st.session_state.messages.append(AIMessage(content=response_content))

        except Exception as e:
            st.error(f"Une erreur est survenue : {e}")
            logging.exception("Erreur lors du processus RAG")

st.markdown("---")
st.caption("Mode RAG pur | Donn√©es index√©es via FAISS")